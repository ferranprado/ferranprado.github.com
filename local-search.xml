<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>线性代数学习</title>
    <link href="/2023/01/07/learn-algre/"/>
    <url>/2023/01/07/learn-algre/</url>
    
    <content type="html"><![CDATA[<p>线性代数<br>概率论<br>微积分</p><p>参考内容如下：</p><p>《深度学习》<br>github.com&#x2F;scutan90&#x2F;De…<br>github.com&#x2F;sladesha&#x2F;Re…</p><p>本文是第一篇，线性代数部分的内容，主要是比较基础部分的学习笔记。</p><ol><li>线性代数<br>1.1 向量和矩阵<br>1.1.1 标量、向量、矩阵、张量之间的联系<br>标量（scalar）<br>一个标量表示一个单独的数，它不同于线性代数中研究的其他大部分对象（通常是多个数的数组）。我们用斜体表示标量。标量通常被赋予小写的变量名称。 一般会明确标量属于哪种类型，比如定义实数标量时，会说“令 s∈Rs\in Rs∈R 表示一条线的斜率”。<br>向量（vector）<br>一个向量表示一组有序排列的数。通过次序中的索引，我们可以确定每个单独的数。通常我们赋予向量粗体的小写变量名称，比如xx。向量中的元素可以通过带脚标的斜体表示。向量XXX的第一个元素是X1X_1X1​，第二个元素是X2X_2X2​，以此类推。我们也会注明存储在向量中的元素的类型（实数、虚数等）。<br>一个向量如下所示，一个向量可以看作空间中的点，即每个元素可以表示不同坐标轴上的坐标。<br>x&#x3D;[x1x2x3⋯xn]x &#x3D; \left[ \begin{matrix} x_1 \ x_2 \ x_3 \ \cdots \ x_n \end{matrix} \right]x&#x3D;⎣⎢⎢⎢⎢⎢⎡​x1​x2​x3​⋯xn​​⎦⎥⎥⎥⎥⎥⎤​<br>矩阵（matrix）<br>矩阵是具有相同特征和纬度的对象的集合，表现为一张二维数据表。其意义是一个对象表示为矩阵中的一行，一个特征表示为矩阵中的一列，每个特征都有数值型的取值。通常会赋予矩阵粗体的大写变量名称，比如AAA。<br>一个矩阵的表示例子如下所示：<br>A&#x3D;[A1,1A1,2A2,1A2,2]A &#x3D; \left[ \begin{matrix} A_{1,1} &amp; A_{1,2} \ A_{2,1} &amp; A_{2,2} \ \end{matrix} \right]A&#x3D;[A1,1​A2,1​​A1,2​A2,2​​]<br>转置是矩阵的重要操作之一，其转置是以对角线为轴的镜像，这条从左上角到右下角的对角线被称为主对角线，定义如下:<br>(AT)i,j&#x3D;Aj,i(A^T){i,j} &#x3D; A_{j,i}(AT)i,j&#x3D;Aj,i​<br>一个示例操作如下：<br>A&#x3D;[A1,1A1,2A2,1A2,2A3,1A3,2]&#x3D;&#x3D;&gt;AT&#x3D;[A1,1A2,1A3,1A1,2A2,2A3,2]A &#x3D; \left[ \begin{matrix} A_{1,1} &amp; A_{1,2} \ A_{2,1} &amp; A_{2,2} \ A_{3,1} &amp; A_{3,2} \end{matrix} \right] &#x3D;&#x3D;&gt; A^T &#x3D; \left[ \begin{matrix} A_{1,1} &amp; A_{2,1} &amp; A_{3, 1} \ A_{1,2} &amp; A_{2,2} &amp; A_{3,2}\ \end{matrix} \right]A&#x3D;⎣⎢⎡​A1,1​A2,1​A3,1​​A1,2​A2,2​A3,2​​⎦⎥⎤​&#x3D;&#x3D;&gt;AT&#x3D;[A1,1​A1,2​​A2,1​A2,2​​A3,1​A3,2​​]<br>从一个 3×23\times 23×2 的矩阵变为了 2×3 2\times 32×3 的矩阵。<br>张量（tensor）<br>在某些情况下，我们会讨论坐标超过两维的数组。一般地，一个数组中的元素分布在若干维坐标的规则网格中，我们将其称之为张量。使用 AAA 来表示张量“A”。张量AAA中坐标为(i,j,k)(i,j,k)(i,j,k)的元素记作A(i,j,k)A_{(i,j,k)}A(i,j,k)​。<br>四者之间关系<br>（来自深度学习 500 问第一章数学基础）</li></ol><p>标量是0阶张量，向量是一阶张量。举例：<br>​标量就是知道棍子的长度，但是你不会知道棍子指向哪儿。<br>​向量就是不但知道棍子的长度，还知道棍子指向前面还是后面。<br>​张量就是不但知道棍子的长度，也知道棍子指向前面还是后面，还能知道这棍子又向上&#x2F;下和左&#x2F;右偏转了多少。</p><p>1.1.2 张量与矩阵的区别</p><p>从代数角度讲， 矩阵它是向量的推广。向量可以看成一维的“表格”（即分量按照顺序排成一排）， 矩阵是二维的“表格”（分量按照纵横位置排列）， 那么nnn阶张量就是所谓的nnn维的“表格”。 张量的严格定义是利用线性映射来描述。<br>从几何角度讲， 矩阵是一个真正的几何量，也就是说，它是一个不随参照系的坐标变换而变化的东西。向量也具有这种特性。<br>张量可以用3×3矩阵形式来表达。<br>表示标量的数和表示向量的三维数组也可分别看作1×1，1×3的矩阵。</p><p>1.1.3 矩阵和向量相乘结果<br>若使用爱因斯坦求和约定（Einstein summation convention），矩阵AAA, BBB相乘得到矩阵 CCC 可以用下式表示：<br>AB&#x3D;C&#x3D;&#x3D;&gt;aik∗bkj&#x3D;cijAB &#x3D; C &#x3D;&#x3D;&gt; a_{ik}*b_{kj}&#x3D;c_{ij} AB&#x3D;C&#x3D;&#x3D;&gt;aik​∗bkj​&#x3D;cij​<br>其中，aika_{ik}aik​, bkjb_{kj}bkj​, cijc_{ij}cij​分别表示矩阵A,B,CA, B, CA,B,C的元素，kkk出现两次，是一个哑变量（Dummy Variables）表示对该参数进行遍历求和。<br>用一个例子表示就是：<br>A&#x3D;[A1,1A1,2A2,1A2,2] B&#x3D;[B1,1B1,2B2,1B2,2]A×B&#x3D;C&#x3D;[A1,1×B1,1+A1,2×B2,1A1,1×B1,2+A1,2×B2,2A2,1×B1,1+A2,2×B2,1A2,1×B1,2+A2,2×B2,2]&#x3D;[C1,1C1,2C2,1C2,2]A&#x3D; \left[ \begin{matrix} A_{1,1} &amp; A_{1,2} \ A_{2,1} &amp; A_{2,2} \ \end{matrix} \right] \ B &#x3D; \left[ \begin{matrix} B_{1,1} &amp; B_{1,2} \ B_{2,1} &amp; B_{2,2} \ \end{matrix} \right] \ A \times B &#x3D; C &#x3D; \left[ \begin{matrix} A_{1,1}\times B_{1,1}+A_{1,2}\times B_{2,1} &amp; A_{1,1}\times B_{1,2}+A_{1,2}\times B_{2,2} \ A_{2,1}\times B_{1,1}+A_{2,2}\times B_{2,1} &amp; A_{2,1}\times B_{1,2}+A_{2,2}\times B_{2,2} \ \end{matrix} \right] &#x3D; \left[ \begin{matrix} C_{1,1} &amp; C_{1,2} \ C_{2,1} &amp; C_{2,2} \ \end{matrix} \right]A&#x3D;[A1,1​A2,1​​A1,2​A2,2​​] B&#x3D;[B1,1​B2,1​​B1,2​B2,2​​]A×B&#x3D;C&#x3D;[A1,1​×B1,1​+A1,2​×B2,1​A2,1​×B1,1​+A2,2​×B2,1​​A1,1​×B1,2​+A1,2​×B2,2​A2,1​×B1,2​+A2,2​×B2,2​​]&#x3D;[C1,1​C2,1​​C1,2​C2,2​​]<br>所以矩阵相乘有一个前提，矩阵 A 的列数必须和矩阵 B 的行数相等，也就是如果 A 的维度是 m×nm\times nm×n，B 的维度必须是 n×pn \times pn×p，相乘得到的 C 矩阵的维度就是 m×pm\times pm×p。<br>另外还有一种矩阵乘法，是矩阵对应元素相乘，这种称为元素对应乘积，或者 Hadamard 乘积，记为 A ⊙ B<br>而矩阵和向量相乘可以看成是矩阵相乘的一个特殊情况，例如：矩阵BBB是一个n×1n \times 1n×1的矩阵。<br>矩阵乘积满足这些定律：</p><p>服从分配率：A(B+C) &#x3D; AB + AC<br>服从结合律：A(BC) &#x3D; (AB)C</p><p>但是不服从交换律，即 AB 不一定等于 BA。<br>矩阵的乘积满足：（AB)T&#x3D;ATBT（AB)^T &#x3D; A^TB^T（AB)T&#x3D;ATBT<br>两个相同维度的向量 x 和 y 的点积(dot product)，可以看作矩阵乘积–xTyx^TyxTy。也就是说可以将矩阵乘积 C&#x3D;ABC&#x3D;ABC&#x3D;AB 中计算 Ci,jC_{i,j}Ci,j​的步骤看作是 A 的第 i 行和 B 的第 j 列之间的点积。毕竟，矩阵的每一行或者每一列都是一个向量。<br>而向量的点积是满足交换律的：<br>xTy&#x3D;yTxx^Ty &#x3D; y^TxxTy&#x3D;yTx<br>证明主要是根据：</p><p>两个向量的点积是标量<br>标量的转置也是自身</p><p>所以有：<br>xTy&#x3D;(xTy)T&#x3D;xyTx^Ty &#x3D; (x^Ty)^T &#x3D; xy^TxTy&#x3D;(xTy)T&#x3D;xyT<br>1.1.4 单位矩阵和逆矩阵<br>单位矩阵的定义如下，用 I 表示单位矩阵，任何向量和单位矩阵相乘，都不会改变，即：<br>∀x∈Rn,Inx&#x3D;x(1-1-8)\forall x \in R^n, I_n x &#x3D; x \tag{1-1-8}∀x∈Rn,In​x&#x3D;x(1-1-8)<br>单位矩阵的结构很简单，就是主对角线是 1，其他位置是 0，如下图所示的单位矩阵 I3I_3I3​ ：<br>[100010001]\left[ \begin{matrix} 1 &amp; 0 &amp; 0 \ 0 &amp; 1 &amp; 0 \ 0 &amp; 0 &amp; 1 \end{matrix} \right]⎣⎢⎡​100​010​001​⎦⎥⎤​<br>而逆矩阵记作 A−1A^{-1}A−1，其满足如下条件：<br>A−1A&#x3D;InA^{-1}A&#x3D;I_nA−1A&#x3D;In​<br>1.1.5 线性方程组和线性相关<br>现在有一个线性方程组，如下所示：<br>Ax&#x3D;bAx &#x3D; bAx&#x3D;b<br>其中，A∈Rm×nA\in R^{m\times n}A∈Rm×n 是已知的矩阵，b∈Rmb\in R^mb∈Rm 是已知的向量，然后 x∈Rnx\in R^nx∈Rn 是需要求解的未知向量。<br>这里根据矩阵相乘（x 相当于一个 n×1n\times 1n×1 的矩阵），可以将上述公式拓展开来：<br>A1,:x&#x3D;b1&#x3D;&#x3D;&gt;A1,1x1+A1,2x2+⋯+A1,nxn&#x3D;b1A2,:x&#x3D;b2&#x3D;&#x3D;&gt;A2,1x1+A2,2x2+⋯+A2,nxn&#x3D;b2⋯Am,:x&#x3D;bm&#x3D;&#x3D;&gt;Am,1x1+Am,2x2+⋯+Am,nxn&#x3D;bmA_{1,:}x &#x3D; b_1 &#x3D;&#x3D;&gt; A_{1,1}x_1 + A_{1,2}x_2+\cdots+A_{1,n}x_n &#x3D; b_1 \ A_{2,:}x &#x3D; b_2 &#x3D;&#x3D;&gt; A_{2,1}x_1 + A_{2,2}x_2+\cdots+A_{2,n}x_n &#x3D; b_2 \ \cdots \ A_{m,:}x &#x3D; b_m &#x3D;&#x3D;&gt; A_{m,1}x_1 + A_{m,2}x_2+\cdots+A_{m,n}x_n &#x3D; b_m \A1,:​x&#x3D;b1​&#x3D;&#x3D;&gt;A1,1​x1​+A1,2​x2​+⋯+A1,n​xn​&#x3D;b1​A2,:​x&#x3D;b2​&#x3D;&#x3D;&gt;A2,1​x1​+A2,2​x2​+⋯+A2,n​xn​&#x3D;b2​⋯Am,:​x&#x3D;bm​&#x3D;&#x3D;&gt;Am,1​x1​+Am,2​x2​+⋯+Am,n​xn​&#x3D;bm​<br>在我们定义了逆矩阵后，那么可以这么求解：<br>Ax&#x3D;bA−1Ax&#x3D;A−1bInx&#x3D;A−1bx&#x3D;A−1bAx&#x3D;b\ A^{-1}Ax &#x3D; A^{-1}b\ I_nx &#x3D; A^{-1}b \ x &#x3D; A^{-1}bAx&#x3D;bA−1Ax&#x3D;A−1bIn​x&#x3D;A−1bx&#x3D;A−1b<br>所以求解的关键就是是否存在一个逆矩阵，并找到它。<br>当逆矩阵A−1A^{-1}A−1存在的时候，对每个向量 b 肯定恰好存在一个解。<br>但对于方程组来说，向量 b 的某些值，有可能不存在解，或者有无限多个解，不存在多于1 个解，但有限解的情况，比如 x 和 y 都是方程组的解，则有：<br>z&#x3D;αx+(1−α)yz &#x3D; \alpha x + (1-\alpha)yz&#x3D;αx+(1−α)y<br>其中，α\alphaα 是任意实数，那么 z 也是方程组的解，这种组合是无限的，所以不存在有限解（多于 1 个）。<br>确定 Ax&#x3D;b 是否有解，关键是确定向量 b 是否在 A 列向量的生成子空间中，这个特殊的生成子空间，被称为 A 的列空间或者 A 的值域。</p><p>一组向量的线性组合是指每个向量乘以对应标量系数之后的和，即 ∑iciv(i)\sum_i c_i v^{(i)}∑i​ci​v(i)<br>一组向量的生成子空间是原始向量线性组合后所能抵达的点的集合。</p><p>那么为了让上述成立，应该让 A 的列空间构成整个 RmR^mRm 空间，如果这个空间某个点不在 A 的列空间，那么对应的 b 会使得方程无解。而要让其成立，**即要满足不等式 n≥mn\ge mn≥m **。<br>但该不等式只是方程对每个 b 有解的必要条件，非充分条件。因为存在一种情况，某些列向量可能是冗余的，比如一个 2×22\times 22×2的矩阵，如果两个列向量都是相同的，那该矩阵的列空间和它的一个列向量作为矩阵的列空间是一样的，并不能满足覆盖了整个 R2R^2R2 空间。<br>这种冗余也被称为线性相关，而如果一组向量中任意一个向量都不能表示为其他向量的线性组合，则这组向量称为线性无关。<br>所以，如果一个矩阵的列空间要覆盖整个 RmR^mRm，那么该矩阵必须包含至少一组m 个线性无关的向量，这才是对每个 b 都有解的充分必要条件。<br>此外，要让矩阵可逆，还必须保证 Ax&#x3D;b 对每个 b 的取值至多只有一个解，那必须保证该矩阵至多有 m 个列向量，否则方程有不止一个解。<br>综上，那么矩阵就必须是方阵，也就是 m &#x3D; n，并且所有列向量都是线性无关的。一个列向量都是线性无关的方阵被称为是奇异的。<br>假如 A 不是方阵或者不是奇异的方阵，也可能有解，但是不能通过逆矩阵去求解。<br>1.1.6 向量和矩阵的范数归纳<br>向量的范数(norm)<br>通常衡量向量的大小是通过范数来衡量的，形式上 LPL^PLP范数定义如下：<br>Lp&#x3D;∥x⃗∥p&#x3D;∑i&#x3D;1N∣xi∣ppL_p&#x3D;\Vert\vec{x}\Vert_p&#x3D;\sqrt[p]{\sum_{i&#x3D;1}^{N}|{x_i}|^p}Lp​&#x3D;∥x∥p​&#x3D;pi&#x3D;1∑N​∣xi​∣p​<br>这里 p≥1p\ge 1p≥1。<br>范数是将向量映射到非负数的函数，直观上来说，向量 x 的范数衡量从原点到点 x 的距离。<br>范数是满足下列性质的任意函数：<br>f(x)&#x3D;0&#x3D;&gt;x&#x3D;0f(x+y)≤f(x)+f(y)(三角不等式)∀α∈R,f(αx)&#x3D;∣α∣f(x)f(x)&#x3D;0&#x3D;&gt;x&#x3D;0 \ f(x+y)\le f(x)+f(y)(三角不等式)\ \forall \alpha \in R, f(\alpha x) &#x3D; |\alpha|f(x)f(x)&#x3D;0&#x3D;&gt;x&#x3D;0f(x+y)≤f(x)+f(y)(三角不等式)∀α∈R,f(αx)&#x3D;∣α∣f(x)<br>定义一个向量为：a⃗&#x3D;[−5,6,8,−10]\vec{a}&#x3D;[-5, 6, 8, -10]a&#x3D;[−5,6,8,−10]。任意一组向量设为x⃗&#x3D;(x1,x2,…,xN)\vec{x}&#x3D;(x_1,x_2,…,x_N)x&#x3D;(x1​,x2​,…,xN​)。其不同范数求解如下：</p><p>向量的1范数：向量的各个元素的绝对值之和，上述向量a⃗\vec{a}a的1范数结果就是：x &#x3D; |-5|+|6|+|8|+|-10| &#x3D; 29。</p><p>∥x⃗∥1&#x3D;∑i&#x3D;1N∣xi∣\Vert\vec{x}\Vert_1&#x3D;\sum_{i&#x3D;1}^N\vert{x_i}\vert∥x∥1​&#x3D;i&#x3D;1∑N​∣xi​∣</p><p>向量的2范数（欧几里得范数）：向量的每个元素的平方和再开平方根，上述a⃗\vec{a}a的2范数结果就是：x&#x3D;(−5)2+(6)2+(8)2+(−10)215x&#x3D;\sqrt{(-5)^2+(6)^2+(8)^2+(-10)^2}15x&#x3D;(−5)2+(6)2+(8)2+(−10)2​15。</p><p>∥x⃗∥2&#x3D;∑i&#x3D;1N∣xi∣2\Vert\vec{x}\Vert_2&#x3D;\sqrt{\sum_{i&#x3D;1}^N{\vert{x_i}\vert}^2}∥x∥2​&#x3D;i&#x3D;1∑N​∣xi​∣2​</p><p>向量的负无穷范数：向量的所有元素的绝对值中最小的：上述向量a⃗\vec{a}a的负无穷范数结果就是：5。</p><p>∥x⃗∥−∞&#x3D;min⁡∣xi∣\Vert\vec{x}\Vert_{-\infty}&#x3D;\min{|{x_i}|}∥x∥−∞​&#x3D;min∣xi​∣</p><p>向量的正无穷范数：向量的所有元素的绝对值中最大的：上述向量a⃗\vec{a}a的正无穷范数结果就是：10。</p><p>∥x⃗∥+∞&#x3D;max⁡∣xi∣\Vert\vec{x}\Vert_{+\infty}&#x3D;\max{|{x_i}|}∥x∥+∞​&#x3D;max∣xi​∣<br>矩阵的范数<br>定义一个矩阵。<br>A&#x3D;[−12−34−66]A &#x3D; \left[ \begin{matrix} -1 &amp; 2 &amp; -3 \ 4 &amp; -6 &amp; 6 \ \end{matrix} \right]A&#x3D;[−14​2−6​−36​]<br>任意矩阵定义为：Am×nA_{m\times n}Am×n​，其元素为 aija_{ij}aij​。<br>矩阵的范数定义为<br>∥A∥p:&#x3D;sup⁡x≠0∥Ax∥p∥x∥p\Vert{A}\Vert_p :&#x3D;\sup_{x\neq 0}\frac{\Vert{Ax}\Vert_p}{\Vert{x}\Vert_p}∥A∥p​:&#x3D;x&#x3D;0sup​∥x∥p​∥Ax∥p​​<br>当向量取不同范数时, 相应得到了不同的矩阵范数。</p><p>矩阵的1范数（列范数）：先对矩阵的每一列元素的绝对值求和，再从中取个最大的（列和最大），上述矩阵AAA的1范数先得到[5,8,9][5,8,9][5,8,9]，再取最大的最终结果就是：9。</p><p>∥A∥1&#x3D;max⁡1≤j≤n∑i&#x3D;1m∣aij∣\Vert A\Vert_1&#x3D;\max_{1\le j\le n}\sum_{i&#x3D;1}^m|{a_{ij}}|∥A∥1​&#x3D;1≤j≤nmax​i&#x3D;1∑m​∣aij​∣</p><p>矩阵的2范数：矩阵ATAA^TAATA的最大特征值开平方根，上述矩阵AAA的2范数得到的最终结果是：10.0623。</p><p>∥A∥2&#x3D;λmax(ATA)\Vert A\Vert_2&#x3D;\sqrt{\lambda_{max}(A^T A)}∥A∥2​&#x3D;λmax​(ATA)​<br>其中， λmax(ATA)\lambda_{max}(A^T A)λmax​(ATA) 为 ATAA^T AATA 的特征值绝对值的最大值。</p><p>矩阵的无穷范数（行范数）：矩阵的每一行上的元素绝对值先求和，再从中取个最大的，（行和最大），上述矩阵AAA的行范数先得到[6；16][6；16][6；16]，再取最大的最终结果就是：16。</p><p>∥A∥∞&#x3D;max⁡1≤i≤m∑j&#x3D;1n∣aij∣\Vert A\Vert_{\infty}&#x3D;\max_{1\le i \le m}\sum_{j&#x3D;1}^n |{a_{ij}}|∥A∥∞​&#x3D;1≤i≤mmax​j&#x3D;1∑n​∣aij​∣</p><p>矩阵的核范数：矩阵的奇异值（将矩阵svd分解）之和，这个范数可以用来低秩表示（因为最小化核范数，相当于最小化矩阵的秩——低秩），上述矩阵A最终结果就是：10.9287。</p><p>矩阵的L0范数：矩阵的非0元素的个数，通常用它来表示稀疏，L0范数越小0元素越多，也就越稀疏，上述矩阵AAA最终结果就是：6。</p><p>矩阵的L1范数：矩阵中的每个元素绝对值之和，它是L0范数的最优凸近似，因此它也可以表示稀疏，上述矩阵AAA最终结果就是：22。</p><p>矩阵的F范数：最常用的矩阵的范数，矩阵的各个元素平方之和再开平方根，它通常也叫做矩阵的L2范数，它的优点在于它是一个凸函数，可以求导求解，易于计算，上述矩阵A最终结果就是：10.0995。</p><p>∥A∥F&#x3D;(∑i&#x3D;1m∑j&#x3D;1n∣aij∣2)\Vert A\Vert_F&#x3D;\sqrt{(\sum_{i&#x3D;1}^m\sum_{j&#x3D;1}^n{| a_{ij}|}^2)}∥A∥F​&#x3D;(i&#x3D;1∑m​j&#x3D;1∑n​∣aij​∣2)​</p><p>矩阵的L21范数：矩阵先以每一列为单位，求每一列的F范数（也可认为是向量的2范数），然后再将得到的结果求L1范数（也可认为是向量的1范数），很容易看出它是介于L1和L2之间的一种范数，上述矩阵AAA最终结果就是：17.1559。<br>矩阵的 p范数</p><p>∥A∥p&#x3D;(∑i&#x3D;1m∑j&#x3D;1n∣aij∣p)p\Vert A\Vert_p&#x3D;\sqrt[p]{(\sum_{i&#x3D;1}^m\sum_{j&#x3D;1}^n{| a_{ij}|}^p)}∥A∥p​&#x3D;p(i&#x3D;1∑m​j&#x3D;1∑n​∣aij​∣p)​<br>两个向量的点积可以用范数来表示：<br>xTy&#x3D;∥x∥2∥y∥2cosθx^Ty &#x3D;\Vert x \Vert_2 \Vert y \Vert_2 cos\thetaxTy&#x3D;∥x∥2​∥y∥2​cosθ<br>这里 θ\thetaθ 就是 x 和 y 之间的夹角。<br>1.1.7 一些特殊的矩阵和向量<br>对角矩阵：只在对角线上有非零元素，其他位置都是零。之前介绍的单位矩阵就是对角矩阵的一种；<br>对称矩阵：转置和自己相等的矩阵，即：A&#x3D;ATA &#x3D; A^TA&#x3D;AT。<br>单位向量：具有单位范数的向量，也就是 ∥x∥2&#x3D;1\Vert x \Vert_2 &#x3D;1∥x∥2​&#x3D;1<br>向量正交：如果 xTy&#x3D;0x^Ty&#x3D;0xTy&#x3D;0，那么就说向量 x 和 y 互相正交。如果向量不仅互相正交，范数还是 1，那么就称为标准正交。<br>正交矩阵：行向量和列向量是分别标准正交的方阵，即<br>ATA&#x3D;AAT&#x3D;IA^TA&#x3D;AA^T&#x3D;IATA&#x3D;AAT&#x3D;I<br>也就是有：<br>A−1&#x3D;ATA^{-1}&#x3D;A^TA−1&#x3D;AT<br>所以正交矩阵的一个优点就是求逆计算代价小。<br>1.1.8 如何判断一个矩阵为正定<br>判定一个矩阵是否为正定，通常有以下几个方面：</p><p>顺序主子式全大于0；<br>存在可逆矩阵CCC使CTCC^TCCTC等于该矩阵；<br>正惯性指数等于nnn；<br>合同于单位矩阵EEE（即：规范形为EEE）<br>标准形中主对角元素全为正；<br>特征值全为正；<br>是某基的度量矩阵。</p><p>所有特征值是非负数的矩阵称为半正定，而所有特征值是负数的矩阵称为负定，所有特征值是非正数的矩阵称为半负定。<br>正定性的用途</p><p>Hessian矩阵正定性在梯度下降的应用</p><p>若Hessian正定,则函数的二阶偏导恒大于0，,函数的变化率处于递增状态，判断是否有局部最优解</p><p>在 svm 中核函数构造的基本假设</p><p>1.2 特征值和特征向量<br>1.2.1 特征值分解与特征向量<br>特征分解是使用最广的矩阵分解之一，矩阵分解可以得到一组特征值(eigenvalues)与特征向量(eigenvectors)；<br>特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么。<br>如果说一个向量v⃗\vec{v}v是方阵AAA的特征向量，将一定可以表示成下面的形式：<br>Aν&#x3D;λνA\nu &#x3D; \lambda \nuAν&#x3D;λν<br>λ\lambdaλ为特征向量v⃗\vec{v}v对应的特征值。<br>特征值分解是将一个矩阵分解为如下形式：<br>A&#x3D;Q∑Q−1A&#x3D;Q\sum Q^{-1}A&#x3D;Q∑Q−1<br>其中，QQQ是这个矩阵AAA的特征向量组成的正交矩阵，∑\sum∑是一个对角矩阵，每一个对角线元素就是一个特征值，里面的特征值是由大到小排列的，这些特征值所对应的特征向量就是描述这个矩阵变化方向（从主要的变化到次要的变化排列）。也就是说矩阵AAA的信息可以由其特征值和特征向量表示。<br>并非每个矩阵都可以分解成特征值和特征向量，但每个实对称矩阵都可以分解为实特征向量和实特征值。<br>1.2.2 奇异值分解<br>除了特征分解外，还有一种矩阵分解，称为奇异值分解（SVD)，将矩阵分解为奇异值和奇异向量。通过奇异值分解，可以得到和特征分解相同类型的信息，但是，奇异值分解有更广泛的应用，每个实数矩阵都有一个奇异值分解，但不一定有特征分解，因为必须是方阵才有特征分解。<br>在特征分解中，我们将 A 重新写作：<br>A&#x3D;Vdiag(λ)V−1A &#x3D; Vdiag(\lambda)V^{-1}A&#x3D;Vdiag(λ)V−1<br>其中，V 是特征向量构成的矩阵，λ\lambdaλ是特征值构成的向量，diag(λ)diag(\lambda)diag(λ)表示一个对角线都是特征值的对角矩阵。<br>奇异值分解的形式如下所示：<br>A&#x3D;UDVTA &#x3D; U D V^TA&#x3D;UDVT<br>假如 A 是 m×nm\times nm×n 的矩阵，则 U 是 m×mm\times mm×m的矩阵，D 是 m×nm\times nm×n 的矩阵，V 是 n×nn\times nn×n 的矩阵。并且，矩阵 U 和 V 是正交矩阵，D 是对角矩阵，且不一定是方阵。<br>D 对角线上的元素就是 A 的奇异值，而 U 的列向量是左奇异向量，V 的列向量是右奇异向量。<br>可以套用和 A 相关的特征分解来解释其奇异值分解，A 的左奇异向量就是 AATAA^TAAT的特征向量，而右奇异向量就是ATAA^TAATA 的特征向量，A 的非零奇异值是AATAA^TAAT特征值的平方根，也是ATAA^TAATA特征值的平方根。<br>(来自深度学习 500 问的数学基础的内容)</p><p>那么奇异值和特征值是怎么对应起来的呢？我们将一个矩阵AAA的转置乘以AAA，并对ATAA^TAATA求特征值，则有下面的形式：<br>(ATA)V&#x3D;λV(A^TA)V &#x3D; \lambda V(ATA)V&#x3D;λV<br>这里VVV就是上面的右奇异向量，另外还有：<br>σi&#x3D;λi,ui&#x3D;1σiAV\sigma_i &#x3D; \sqrt{\lambda_i}, u_i&#x3D;\frac{1}{\sigma_i}AVσi​&#x3D;λi​​,ui​&#x3D;σi​1​AV<br>这里的σ\sigmaσ就是奇异值，uuu就是上面说的左奇异向量。</p><p>奇异值σ\sigmaσ跟特征值类似，在矩阵∑\sum∑中也是从大到小排列，而且σ\sigmaσ的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上了。也就是说，我们也可以用前rrr（rrr远小于m、nm、nm、n）个的奇异值来近似描述矩阵，即部分奇异值分解：<br>Am×n≈Um×r∑r×rVr×nTA_{m\times n}\approx U_{m \times r}\sum_{r\times r}V_{r \times n}^TAm×n​≈Um×r​r×r∑​Vr×nT​<br>右边的三个矩阵相乘的结果将会是一个接近于AAA的矩阵，在这儿，rrr越接近于nnn，则相乘的结果越接近于AAA。</p><p>作者：AI算法笔记<br>链接：<a href="https://juejin.cn/post/6882194485061779470">https://juejin.cn/post/6882194485061779470</a><br>来源：稀土掘金<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
